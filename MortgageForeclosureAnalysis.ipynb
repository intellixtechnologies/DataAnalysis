{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import settings as st\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4579594, 25)\n"
     ]
    },
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-acf45b4cfc9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Acquisition\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Performance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-acf45b4cfc9a>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0merror_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 low_memory=False)\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mread_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSELECT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mfull_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhis\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    643\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhis\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhis\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\abhis\\Anaconda3\\envs\\DAND\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1505\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1508\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas\\parser.c:10387)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas\\parser.c:11556)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas\\parser.c:26979)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "'''\n",
    "*************** STAGE 1: DATA GATHERING ***********************\n",
    "Assemble all the different .txt files (Aquisition and Performance) into 2 files :\n",
    "We’ll first need to define the headers for each file\n",
    "https://loanperformancedata.fanniemae.com/lppub-docs/lppub_file_layout.pdf\n",
    "'''\n",
    "\n",
    "HEADERS = {\n",
    "    \"Acquisition\": [\n",
    "        \"id\", \"channel\", \"seller\", \"interest_rate\", \"balance\", \"loan_term\",\n",
    "        \"origination_date\", \"first_payment_date\", \"ltv\", \"cltv\",\n",
    "        \"borrower_count\", \"dti\", \"borrower_credit_score\",\n",
    "        \"first_time_homebuyer\", \"loan_purpose\", \"property_type\", \"unit_count\",\n",
    "        \"occupancy_status\", \"property_state\", \"zip\", \"insurance_percentage\",\n",
    "        \"product_type\", \"co_borrower_credit_score\",\"mortgage_insurance_type\",\"relocation_mortgage_indicator\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"id\", \"reporting_period\", \"servicer_name\", \"interest_rate\", \"balance\",\n",
    "        \"loan_age\", \"months_to_maturity\", \"maturity_date\", \"msa\",\n",
    "        \"delinquency_status\", \"modification_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_date\", \"last_paid_installment_date\", \"foreclosure_date\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"property_repair_costs\",\n",
    "        \"recovery_costs\", \"misc_costs\", \"tax_costs\", \"sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_proceeds\",\n",
    "        \"other_foreclosure_proceeds\", \"non_interest_bearing_balance\",\n",
    "        \"principal_forgiveness_balance\",\"REPURCHASE_MAKE_WHOLE_PROCEEDS_FLAG\",\n",
    "        \"FORECLOSURE_PRINCIPAL_WRITE_OFF_AMOUNT\",\"SERVICING_ACTIVITY_INDICATOR\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# We will now define the columns we want to keep. Since all we’re measuring on an ongoing basis about the loan is\n",
    "# whether or not it was ever foreclosed on, we can discard many of the columns in the performance data.\n",
    "# We’ll need to keep all the columns in the acquisition data, though, because we want to maximize the\n",
    "# information we have about when the loan was acquired\n",
    "\n",
    "SELECT = {\n",
    "    \"Acquisition\": HEADERS[\"Acquisition\"],\n",
    "    \"Performance\": [\"id\", \"foreclosure_date\"]\n",
    "}\n",
    "\n",
    "\n",
    "def concatenate(prefix=\"Acquisition\"):\n",
    "    dir_files = os.listdir(st.DATA_DIR)\n",
    "    full_file = []\n",
    "    # concatenate all the files in the directory\n",
    "\n",
    "    for f in dir_files:\n",
    "        if f.startswith(prefix):\n",
    "            read_file = pd.read_csv(\n",
    "                os.path.join(st.DATA_DIR, f),\n",
    "                sep='|',\n",
    "                header=None,\n",
    "                names=HEADERS[prefix],\n",
    "                index_col=False,\n",
    "                error_bad_lines=False,\n",
    "                low_memory=False)\n",
    "            read_file = read_file[SELECT[prefix]]\n",
    "            full_file.append(read_file)\n",
    "        else:\n",
    "            continue\n",
    "    full_file = pd.concat(full_file, axis=0)\n",
    "\n",
    "    test = full_file.head(5)\n",
    "    # get shape of the file\n",
    "    print(full_file.shape)\n",
    "\n",
    "    # convert the processed files -\n",
    "    full_file.to_csv(\n",
    "        os.path.join(st.PROCESSED_DIR, \"{}.csv\".format(prefix)), index=False)\n",
    "\n",
    "#concatenate()\n",
    "if __name__ == \"__main__\":\n",
    "    concatenate(\"Acquisition\")\n",
    "    concatenate(\"Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "******************** STAGE 2: DATA CLEANING AND TRANSFORMATION ***************\n",
    "Function to counts number of performance rows for each Loan Id.\n",
    "counts dictionary: get the count of id and their count of occurance\n",
    "'''\n",
    "\n",
    "\n",
    "def count_performance_rows():\n",
    "    foreclosure_counts = {}\n",
    "    # read data from files\n",
    "    # NOTE: Opening a file handler to read rather than Pandas.\n",
    "    # Reason : We want to re\n",
    "    with open(os.path.join(st.PROCESSED_DIR, \"Performance.csv\"), \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            loan_id, date = line.split(\",\")\n",
    "            loan_id = int(loan_id)\n",
    "            if loan_id not in foreclosure_counts:\n",
    "                foreclosure_counts[loan_id] = {\n",
    "                    \"foreclosure_status\": 0,\n",
    "                    \"performance_count\": 0\n",
    "                }  # initialize dict\n",
    "\n",
    "            foreclosure_counts[loan_id][\"performance_count\"] += 1\n",
    "            if len(date.strip()) > 0:\n",
    "                foreclosure_counts[loan_id][\"foreclosure_status\"] = 1\n",
    "    return foreclosure_counts\n",
    "\n",
    "\n",
    "'''\n",
    "Function to extract values from the dictionary if a loan_id and a key are passed in\n",
    "This function will enable us to assign a foreclosure_status value and a performance_count value to each row in the Acquisition data.\n",
    "'''\n",
    "\n",
    "\n",
    "def get_summary_of_performance(loan_id, key, foreclosure_count_dictionary):\n",
    "    #The get method on dictionaries returns a default value if a key isn’t found.\n",
    "    summary_value = foreclosure_count_dictionary.get(\n",
    "    loan_id, {\n",
    "            \"foreclosure_status\": 0,\n",
    "            \"performance_count\": 0\n",
    "        })\n",
    "    return summary_value[key]\n",
    "\n",
    "\n",
    "'''\n",
    "This step will involve:\n",
    "A. data transformation -\n",
    "- Converting all columns to numeric.\n",
    "- Filling in any missing values.\n",
    "- Assigning a performance_count and a foreclosure_status to each row.\n",
    "- Removing any rows that don’t have a lot of performance history (where performance_count is low).\n",
    "B. data cleaning - \n",
    "This will prepare training dataset that can be used in a machine learning algorithm.\n",
    "There are few different category codes, like R, S, that will be converted to 1 , 2.\n",
    "For columns that contain dates we will split them into 2 columns - Month , Year\n",
    "'''\n",
    "\n",
    "\n",
    "def data_transform(acquisition, counts):\n",
    "    # add \"foreclosure_status\" column in acquisition dataframe by getting the values from the counts dictionary\n",
    "    acquisition[\"foreclosure_status\"] = acquisition[\"id\"].apply(\n",
    "    lambda x: get_summary_of_performance(x, \"foreclosure_status\", counts))\n",
    "\n",
    "    # add \"performance_count\" column in acquisition dataframe by getting the values from the counts dictionary.\n",
    "    acquisition[\"performance_count\"] = acquisition[\"id\"].apply(\n",
    "    lambda x: get_summary_of_performance(x, \"performance_count\", counts))\n",
    "\n",
    "    # convert following columns to int - These are category variables\n",
    "    #[\"channel\",\"seller\",\"first_time_homebuyer\",\"loan_purpose\",\"property_type\",\"occupancy_status\",\"property_state\",\"product_type\"]\n",
    "    string_columns = [\n",
    "        \"channel\", \"seller\", \"first_time_homebuyer\", \"loan_purpose\",\n",
    "        \"property_type\", \"occupancy_status\", \"property_state\", \"product_type\"\n",
    "    ]\n",
    "    for column in string_columns:\n",
    "        acquisition[column] = acquisition[column].astype(\"category\").cat.codes\n",
    "\n",
    "    # convert date values - \"first_payment_date\" and \"origination_date\"\n",
    "    for date in [\"first_payment\", \"origination\"]:\n",
    "        cols = \"{}_date\".format(date)\n",
    "        acquisition[\"{}_month\".format(date)] = pd.to_numeric(\n",
    "            acquisition[cols].str.split('/').str.get(0))\n",
    "        acquisition[\"{}_year\".format(date)] = pd.to_numeric(\n",
    "            acquisition[cols].str.split('/').str.get(1))\n",
    "\n",
    "    acquisition = acquisition.fillna(-1)\n",
    "    acquisition = acquisition[\n",
    "        acquisition[\"performance_count\"] > st.MINIMUM_TRACKING_QUARTERS]\n",
    "    return acquisition\n",
    "\n",
    "\n",
    "'''\n",
    "Read the Acquisition dataset\n",
    "'''\n",
    "\n",
    "\n",
    "def read():\n",
    "    acquisition = pd.read_csv(\n",
    "        os.path.join(st.PROCESSED_DIR, \"Acquisition.csv\"))\n",
    "    return acquisition\n",
    "\n",
    "\n",
    "'''\n",
    "write the training dataset to train.csv\n",
    "'''\n",
    "\n",
    "\n",
    "def write(acquisition):\n",
    "    acquisition.to_csv(\n",
    "        os.path.join(st.PROCESSED_DIR, \"train.csv\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    acquisition = read()\n",
    "    counts = count_performance_rows()\n",
    "    acquisition = data_transform(acquisition, counts)\n",
    "    write(acquisition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction model done\n",
      "Accuracy of the model:0.779409198198\n",
      "False Negatives:0\n",
      "False Positive:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "'''\n",
    "********************* STAGE 3: MAKING PREDICTION ************************\n",
    "The training dataset is very unbalanced with very few loans foreclosed compared to the loans that\n",
    "were not foreclosed. This would make a biased prediction.\n",
    "Therefore to predict foreclosures correctly this imbalance needs to be accounted.\n",
    "False Negative is more dangerous here than False Positive, as False Negative would indicate \n",
    "a risky loan being acquired.\n",
    "Error Metric: FP / FN\n",
    "Classifier : Logistic Regression (sklearn library) to classify the loan. The class needs to be\n",
    "balanced as the dataset is biased to non-foreclosures. use : class_weight = balanced\n",
    "OverFitting : Cross Validation (sklearn library) to counter OverFitting of model.\n",
    "'''\n",
    "\n",
    "\n",
    "def prediction_model(train):\n",
    "    model = LogisticRegression(random_state=1, class_weight=\"balanced\")\n",
    "\n",
    "    predictors = train.columns.tolist()\n",
    "\n",
    "    predictors = [p for p in predictors if p not in st.NON_PREDICTORS]\n",
    "\n",
    "    predictions = cross_validation.cross_val_predict(\n",
    "        model, train[predictors], train[st.TARGET], cv=st.CV_FOLDS)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_error(target, predictions):\n",
    "    return metrics.accuracy_score(target, predictions)\n",
    "\n",
    "\n",
    "def compute_false_negatives(target, predictions):\n",
    "    false_negatives = pd.DataFrame({\n",
    "        \"target\": target,\n",
    "        \"predictions\": predictions\n",
    "    })\n",
    "    neg_rate = false_negatives[(false_negatives[\"target\"] == 1) & (\n",
    "        false_negatives[\"predictions\"] == 0)].shape[0] / (\n",
    "            false_negatives[(false_negatives[\"target\"] == 1)].shape[0] + 1)\n",
    "    return neg_rate\n",
    "\n",
    "\n",
    "def compute_false_positive(target, predictions):\n",
    "    false_positives = pd.DataFrame({\n",
    "        \"target\": target,\n",
    "        \"predictions\": predictions\n",
    "    })\n",
    "    pos_rate = false_positives[(false_positives[\"target\"] == 0) & (\n",
    "        false_positives[\"predictions\"] == 1)].shape[0] / (\n",
    "            false_positives[(false_positives[\"target\"] == 0)].shape[0] + 1)\n",
    "    return pos_rate\n",
    "\n",
    "\n",
    "def read():\n",
    "    train = pd.read_csv(os.path.join(st.PROCESSED_DIR, \"train.csv\"))\n",
    "    return train\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = read()\n",
    "    train = train.drop([\"origination_date\", \"first_payment_date\"], 1)\n",
    "    predictions = prediction_model(train)\n",
    "    print(\"Prediction model done\")\n",
    "    model_error = compute_error(train[st.TARGET], predictions)\n",
    "    FN = compute_false_negatives(train[st.TARGET], predictions)\n",
    "    FP = compute_false_positive(train[st.TARGET], predictions)\n",
    "    print(\"Accuracy of the model:{}\".format(model_error))\n",
    "    print(\"False Negatives:{}\".format(FN))\n",
    "    print(\"False Positive:{}\".format(FP))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
